{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP on MINST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Updating METADATA...\n",
      "INFO: Computing changes...\n",
      "INFO: No packages to install, update or remove\n",
      "INFO: Nothing to be done\n"
     ]
    }
   ],
   "source": [
    "Pkg.update()\n",
    "Pkg.add(\"MNIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0],\n",
       "\n",
       "[7.0,2.0,1.0,0.0,4.0,1.0,4.0,9.0,5.0,9.0  …  7.0,8.0,9.0,0.0,1.0,2.0,3.0,4.0,5.0,6.0])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MNIST\n",
    "features = trainfeatures(1)\n",
    "label = trainlabel(1)\n",
    "\n",
    "trainX, trainY = traindata()\n",
    "testX, testY = testdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "abstract Layer\n",
    "abstract Nonlinearity <: Layer\n",
    "abstract LossCriteria <: Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20-element Array{Float64,1}:\n",
       " 2.66544\n",
       " 2.28954\n",
       " 1.851  \n",
       " 2.20363\n",
       " 2.06145\n",
       " 2.45019\n",
       " 2.94704\n",
       " 3.60682\n",
       " 2.74245\n",
       " 3.21255\n",
       " 2.08556\n",
       " 3.19797\n",
       " 1.9404 \n",
       " 2.1804 \n",
       " 1.90152\n",
       " 3.13517\n",
       " 3.01487\n",
       " 2.75813\n",
       " 2.54263\n",
       " 2.95138"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the Fully Connected layers\n",
    "type FCLayer <: Layer\n",
    "    W           :: Array{Float64}\n",
    "    last_input  :: Array{Float64}\n",
    "    last_output :: Array{Float64}\n",
    "    last_loss   :: Array{Float64}\n",
    "\n",
    "    function FCLayer(i, o)\n",
    "        return new(rand(o,i), zeros(i), zeros(o), zeros(o))\n",
    "    end\n",
    "end\n",
    "\n",
    "function forward(l::FCLayer, x::Array{Float64,1})\n",
    "    @assert ndims(x) == 1 && size(x) == (size(l.W)[2],)\n",
    "    l.last_input  = x\n",
    "    l.last_output = l.W * x # matrix multiplication\n",
    "    l.last_output\n",
    "end\n",
    "\n",
    "function backward(l::FCLayer, loss::Array{Float64,1})\n",
    "    @assert size(loss) == (size(l.W)[1],)\n",
    "    l.last_loss = loss\n",
    "    l.W'*loss \n",
    "end\n",
    "\n",
    "function gradient(l::FCLayer)\n",
    "    @assert size(loss) == (size(l.W)[1],)\n",
    "    l.W .* l.last_loss\n",
    "end\n",
    "\n",
    "function getParam(l::FCLayer)\n",
    "    l.W\n",
    "end\n",
    "\n",
    "function setParam(l::FCLayer, theta::Array{Float64})\n",
    "    @assert size(l.W) == size(theta)\n",
    "    l.W = theta\n",
    "end\n",
    "\n",
    "l = FCLayer(10,20)\n",
    "forward(l, rand(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0,0.0,0.0,2.0]\n",
      "[3.0,0.0,0.0,2.0]\n"
     ]
    }
   ],
   "source": [
    "# Define the ReLu layers\n",
    "type ReLu <: Nonlinearity\n",
    "    alpha       :: Float64\n",
    "    last_input  :: Array{Float64}\n",
    "    last_output :: Array{Float64}\n",
    "    last_loss   :: Array{Float64}\n",
    "    function ReLu(alpha::Float64 = 1.0)\n",
    "        @assert alpha >= 0.\n",
    "        return new(alpha, Float64[], Float64[], Float64[])\n",
    "    end\n",
    "end\n",
    "\n",
    "function forward(l::ReLu, x::Array{Float64})\n",
    "    l.last_input  = x\n",
    "    l.last_output = map(y -> max(0., y*l.alpha), x)\n",
    "    l.last_output\n",
    "end\n",
    "\n",
    "function backward(l::ReLu, loss::Array{Float64})\n",
    "    @assert size(l.last_input) == size(loss)\n",
    "    l.last_loss = loss\n",
    "    map(idx -> l.last_input[idx]>=0 ? l.last_input[idx]*l.alpha*loss[idx] : 0., 1:length(l.last_input))\n",
    "end\n",
    "\n",
    "function gradient(l::ReLu)\n",
    "    0\n",
    "end\n",
    "\n",
    "function getParam(l::ReLu)\n",
    "    0\n",
    "end\n",
    "\n",
    "function setParam(l::ReLu, theta::Array{Float64})\n",
    "    nothing\n",
    "end\n",
    "\n",
    "l = ReLu()\n",
    "println(forward(l, [1.,0.,-1.,2.]))\n",
    "println(backward(l, [3.0,2.0,1.,1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4076059644443803\n",
      "([-0.162803,-0.442546,-0.059892],)\n"
     ]
    }
   ],
   "source": [
    "type CrossEntropyLoss <: LossCriteria\n",
    "    last_loss  :: Array{Float64}\n",
    "    last_input :: Array{Float64}\n",
    "    function CrossEntropyLoss()\n",
    "        return new(Float64[], Float64[])\n",
    "    end\n",
    "end    \n",
    "\n",
    "function forward(l::CrossEntropyLoss, y::Array{Float64,1}, label::Array{Float64, 1})\n",
    "    \"\"\"\n",
    "    [label]  label[i] == 1 iff the data is classified to class i\n",
    "    [y]      final input to the loss layer\n",
    "    \"\"\"\n",
    "    class = convert(Int64,label[1])\n",
    "    return -log(e .^ y ./ sum(e .^ y))[class]\n",
    "end\n",
    "\n",
    "function backward(l::CrossEntropyLoss, x::Array{Float64,1}, label::Array{Float64, 1})\n",
    "    \"\"\"\n",
    "    [label]  label[i] == 1 iff the data is classified to class i\n",
    "    [y]      final input to the loss layer\n",
    "    \"\"\"\n",
    "    class = convert(Int64,label[1])\n",
    "    y = e.^x / sum(e.^x)\n",
    "    map(j -> class==j ? y[class]*(1-y[class]) : -y[class]*y[j], indices(x))\n",
    "end\n",
    "l = CrossEntropyLoss()\n",
    "println(forward(l, [1.,2.,0.], [2.]))\n",
    "println(backward(l, [1.,2.,0.], [2.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 4 methods)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract NN\n",
    "type SequentialNet <: NN\n",
    "    layers :: Array{Layer}\n",
    "    lossfn :: LossCriteria\n",
    "    function SequentialNet(layers::Array{Layer}, lossfn::LossCriteria)\n",
    "        return new(layers, lossfn)\n",
    "    end\n",
    "end\n",
    "\n",
    "function forward(net::SequentialNet, x::Array{Float64}, label::Array)\n",
    "    local inp = x\n",
    "    for i = 1:length(net.layers)\n",
    "        inp = forward(net.layers[i], inp)\n",
    "    end\n",
    "    forward(net.lossfn, inp, label)\n",
    "end\n",
    "\n",
    "function backward(net::SequentialNet, label)\n",
    "    dldy = backward(net.lossfn, net.layers[end].last_output, label)\n",
    "    for i = length(net.layers):-1:1\n",
    "        dldy = backward(net.layers[i], dldy)\n",
    "    end\n",
    "    dldy\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialNet(Layer[FCLayer([0.329327 0.997137 … 0.243865 0.825504; 0.104052 0.566994 … 0.397518 0.713275; … ; 0.661024 0.77155 … 0.552315 0.895485; 0.0418732 0.37289 … 0.49831 0.652954],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0  …  0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0  …  0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0  …  0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),ReLu(1.0,Float64[],Float64[],Float64[]),FCLayer([0.0575035 0.744341 … 0.839803 0.589336; 0.0239004 0.309392 … 0.403313 0.580275; … ; 0.951616 0.649983 … 0.668833 0.192077; 0.837551 0.347437 … 0.745485 0.555384],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0  …  0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0  …  0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0  …  0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),ReLu(1.0,Float64[],Float64[],Float64[]),FCLayer([0.203867 0.333672 … 0.861376 0.691244; 0.69312 0.504042 … 0.272233 0.165278; … ; 0.0881577 0.461139 … 0.408118 0.47451; 0.432505 0.186031 … 0.561016 0.561828],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0  …  0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])],CrossEntropyLoss(Float64[],Float64[]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = [\n",
    "    FCLayer(784, 196),\n",
    "    ReLu(),\n",
    "    FCLayer(196, 49),\n",
    "    ReLu(),\n",
    "    FCLayer(49, 10)\n",
    "]\n",
    "criteria = CrossEntropyLoss()\n",
    "net = SequentialNet(layers, criteria)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,784)(60000,)\n",
      "Epo 1:\n",
      "1  64\n",
      "(784,) (1,)\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "LoadError: MethodError: no method matching backward(::FCLayer, ::Tuple{Array{Float64,1}})\nClosest candidates are:\n  backward(::FCLayer, !Matched::Array{Float64,1}) at In[3]:21\n  backward(!Matched::SequentialNet, ::Any) at In[6]:19\nwhile loading In[8], in expression starting on line 42",
     "output_type": "error",
     "traceback": [
      "LoadError: MethodError: no method matching backward(::FCLayer, ::Tuple{Array{Float64,1}})\nClosest candidates are:\n  backward(::FCLayer, !Matched::Array{Float64,1}) at In[3]:21\n  backward(!Matched::SequentialNet, ::Any) at In[6]:19\nwhile loading In[8], in expression starting on line 42",
      "",
      " in backward(::SequentialNet, ::Array{Float64,1}) at ./In[6]:21",
      " in sgd(::SequentialNet, ::Array{Float64,2}, ::Array{Float64,2}, ::Float64) at ./In[8]:8",
      " in train(::SequentialNet, ::Array{Float64,2}, ::Array{Float64,1}) at ./In[8]:30"
     ]
    }
   ],
   "source": [
    "function sgd(net::SequentialNet, batch_X, batch_Y, lr::Float64 = 0.001)\n",
    "    batch_size = size(batch_X)[1]\n",
    "    ttl_loss   = 0.\n",
    "    for b = 1:batch_size\n",
    "        X, Y = batch_X[b,:], batch_Y[b,:]\n",
    "        println(\"$(size(X)) $(size(Y))\")\n",
    "        loss = forward(net, X, Y) # Propogate the input and output, calculate the loss\n",
    "        backward(net, Y) # Propagate the dldy\n",
    "        for l = 1:length(net.layers)\n",
    "            layer = net.layers[l]\n",
    "            setParam(layer, getParam(layer) - lr * gradient(layer) / batch_size )\n",
    "        end\n",
    "        ttl_loss += loss\n",
    "    end\n",
    "    ttl_loss\n",
    "end\n",
    "\n",
    "function train(net::SequentialNet, X, Y)\n",
    "    batch_size, N = 64, size(Y)[1]\n",
    "    batch=0\n",
    "    for epo = 1:100\n",
    "        println(\"Epo $(epo):\")\n",
    "        for bid = 0:ceil(length(X)/batch_size)-1\n",
    "            batch += 1\n",
    "            sidx::Int = convert(Int64, bid*batch_size+1)\n",
    "            eidx::Int = convert(Int64, min(N, (bid+1)*batch_size))\n",
    "            println(\"$(sidx)  $(eidx)\")\n",
    "            batch_X = X[sidx:eidx,:]\n",
    "            batch_Y = Y[sidx:eidx,:]\n",
    "            loss = sgd(net, batch_X, batch_Y)\n",
    "            println(\"[Epo $(epo) : batch $(batch_id)]: loss = $(loss)\")\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "if size(trainX)[1] != 60000\n",
    "    trainX = trainX'\n",
    "end\n",
    "@assert size(trainX)[1] == size(trainY)[1]\n",
    "println(size(trainX), size(trainY))\n",
    "\n",
    "train(net, trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
