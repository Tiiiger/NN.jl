{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP on MINST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Updating METADATA...\n",
      "INFO: Computing changes...\n",
      "INFO: No packages to install, update or remove\n",
      "INFO: Nothing to be done\n"
     ]
    }
   ],
   "source": [
    "Pkg.update()\n",
    "Pkg.add(\"MNIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0],\n",
       "\n",
       "[7.0,2.0,1.0,0.0,4.0,1.0,4.0,9.0,5.0,9.0  …  7.0,8.0,9.0,0.0,1.0,2.0,3.0,4.0,5.0,6.0])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MNIST\n",
    "features = trainfeatures(1)\n",
    "label = trainlabel(1)\n",
    "\n",
    "trainX, trainY = traindata()\n",
    "testX, testY = testdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "abstract Layer\n",
    "abstract Nonlinearity <: Layer\n",
    "abstract LossCriteria <: Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition (::Type{Main.FCLayer})(Any, Any) in module Main at In[54]:9 overwritten at In[55]:9.\n",
      "WARNING: Method definition forward(Main.FCLayer, Array{Float64, 1}) in module Main at In[54]:14 overwritten at In[55]:14.\n",
      "WARNING: Method definition backward(Main.FCLayer, Array{Float64, 1}) in module Main at In[54]:21 overwritten at In[55]:21.\n",
      "WARNING: Method definition gradient(Main.FCLayer) in module Main at In[54]:27 overwritten at In[55]:27.\n",
      "WARNING: Method definition getParam(Main.FCLayer) in module Main at In[54]:32 overwritten at In[55]:32.\n",
      "WARNING: Method definition setParam(Main.FCLayer, Array{Float64, N<:Any}) in module Main at In[54]:36 overwritten at In[55]:36.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FCLayer([0.479665 0.0555237 … 0.913258 0.0387839; 0.0890121 0.664964 … 0.31888 0.77842; … ; 0.547448 0.339538 … 0.408708 0.869741; 0.268743 0.395923 … 0.65829 0.431577],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the Fully Connected layers\n",
    "type FCLayer <: Layer\n",
    "    W           :: Array{Float64}\n",
    "    last_input  :: Array{Float64}\n",
    "    last_output :: Array{Float64}\n",
    "    last_loss   :: Array{Float64}\n",
    "\n",
    "    function FCLayer(i, o)\n",
    "        return new(rand(o,i), zeros(i), zeros(o), zeros(o))\n",
    "    end\n",
    "end\n",
    "\n",
    "function forward(l::FCLayer, x::Array{Float64,1})\n",
    "    @assert size(x) == (size(l.W)[2],)\n",
    "    l.last_input  = x\n",
    "    l.last_output = l.W * x # matrix multiplication\n",
    "    l.last_output\n",
    "end\n",
    "\n",
    "function backward(l::FCLayer, loss::Array{Float64,1})\n",
    "    @assert size(loss) == (size(l.W)[1],)\n",
    "    l.last_loss = loss\n",
    "    l.W'*loss \n",
    "end\n",
    "\n",
    "function gradient(l::FCLayer)\n",
    "    @assert size(loss) == (size(l.W)[1],)\n",
    "    l.W .* l.last_loss\n",
    "end\n",
    "\n",
    "function getParam(l::FCLayer)\n",
    "    l.W\n",
    "end\n",
    "\n",
    "function setParam(l::FCLayer, theta::Array{Float64})\n",
    "    @assert size(l.W) == size(theta)\n",
    "    l.W = theta\n",
    "end\n",
    "\n",
    "FCLayer(10,20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition (::Type{Main.ReLu})() in module Main at In[49]:8 overwritten at In[56]:8.\n",
      "WARNING: Method definition (::Type{Main.ReLu})(Float64) in module Main at In[49]:8 overwritten at In[56]:8.\n",
      "WARNING: Method definition forward(Main.ReLu, Array{Float64, N<:Any}) in module Main at In[49]:14 overwritten at In[56]:14.\n",
      "WARNING: Method definition backward(Main.ReLu, Array{Float64, N<:Any}) in module Main at In[49]:20 overwritten at In[56]:20.\n",
      "WARNING: Method definition gradient(Main.ReLu) in module Main at In[49]:26 overwritten at In[56]:26.\n",
      "WARNING: Method definition getParam(Main.ReLu) in module Main at In[49]:30 overwritten at In[56]:30.\n",
      "WARNING: Method definition setParam(Main.ReLu, Array{Float64, N<:Any}) in module Main at In[49]:34 overwritten at In[56]:34.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ReLu(1.0,Float64[],Float64[],Float64[])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the ReLu layers\n",
    "type ReLu <: Nonlinearity\n",
    "    alpha       :: Float64\n",
    "    last_input  :: Array{Float64}\n",
    "    last_output :: Array{Float64}\n",
    "    last_loss   :: Array{Float64}\n",
    "    function ReLu(alpha::Float64 = 1.0)\n",
    "        @assert alpha >= 0.\n",
    "        return new(alpha, Float64[], Float64[], Float64[])\n",
    "    end\n",
    "end\n",
    "\n",
    "function forward(l::ReLu, x::Array{Float64})\n",
    "    l.last_input  = x\n",
    "    l.last_output = map(y -> max(0., y*l.alpha), x)\n",
    "    l.last_output\n",
    "end\n",
    "\n",
    "function backward(l::ReLu, loss::Array{Float64})\n",
    "    @assert size(l.last_input) == size(loss)\n",
    "    l.last_loss = loss\n",
    "    map(idx -> l.last_input[idx]>=0 ? l.last_input[idx]*l.alpha*loss[idx] : 0., indices(x))\n",
    "end\n",
    "\n",
    "function gradient(l::ReLu)\n",
    "    0\n",
    "end\n",
    "\n",
    "function getParam(l::ReLu)\n",
    "    0\n",
    "end\n",
    "\n",
    "function setParam(l::ReLu, theta::Array{Float64})\n",
    "    nothing\n",
    "end\n",
    "\n",
    "ReLu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition (::Type{Main.CrossEntropyLoss})() in module Main at In[50]:5 overwritten at In[57]:5.\n",
      "WARNING: Method definition forward(Main.CrossEntropyLoss, Array{Float64, 1}, Int32) in module Main at In[50]:10 overwritten at In[57]:10.\n",
      "WARNING: Method definition backward(Main.CrossEntropyLoss, Array{Float64, 1}, Int32) in module Main at In[50]:18 overwritten at In[57]:18.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss(Float64[],Float64[])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type CrossEntropyLoss <: LossCriteria\n",
    "    last_loss  :: Array{Float64}\n",
    "    last_input :: Array{Float64}\n",
    "    function CrossEntropyLoss()\n",
    "        return new(Float64[], Float64[])\n",
    "    end\n",
    "end    \n",
    "\n",
    "function forward(l::CrossEntropyLoss, y::Array{Float64,1}, label::Int32)\n",
    "    \"\"\"\n",
    "    [label]  label[i] == 1 iff the data is classified to class i\n",
    "    [y]      final input to the loss layer\n",
    "    \"\"\"\n",
    "    return -log(e .^ y ./ sum(e .^ y))[label]\n",
    "end\n",
    "\n",
    "function backward(l::CrossEntropyLoss, x::Array{Float64,1}, label::Int32)\n",
    "    \"\"\"\n",
    "    [label]  label[i] == 1 iff the data is classified to class i\n",
    "    [y]      final input to the loss layer\n",
    "    \"\"\"\n",
    "    y = e.^x / sum(e.^x)\n",
    "    map(j -> label==j ? y[label]*(1-y[label]) : -y[label]*y[j], indices(x))\n",
    "end\n",
    "CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition (::Type{Main.SequentialNet})(Array{Main.Layer, N<:Any}, Main.LossCriteria) in module Main at In[51]:6 overwritten at In[58]:6.\n",
      "WARNING: Method definition forward(Main.SequentialNet, Array{Float64, N<:Any}, Array) in module Main at In[51]:11 overwritten at In[58]:11.\n",
      "WARNING: Method definition backward(Main.SequentialNet, Array) in module Main at In[51]:19 overwritten at In[58]:19.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "backward (generic function with 5 methods)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract NN\n",
    "type SequentialNet <: NN\n",
    "    layers :: Array{Layer}\n",
    "    lossfn :: LossCriteria\n",
    "    function SequentialNet(layers::Array{Layer}, lossfn::LossCriteria)\n",
    "        return new(layers, lossfn)\n",
    "    end\n",
    "end\n",
    "\n",
    "function forward(net::SequentialNet, x::Array{Float64}, label::Array)\n",
    "    local inp = x\n",
    "    for i = 1:length(net.layers)\n",
    "        inp = forward(net.layers[i], inp)\n",
    "    end\n",
    "    forward(net.lossfn, inp, label)\n",
    "end\n",
    "\n",
    "function backward(net::SequentialNet, label::Array)\n",
    "    @assert size(dldy) == size(net.inputs[end])\n",
    "    dldy = backward(net.lossfn, net.inputs[end], label)\n",
    "    for i = length(net.layers):-1:1\n",
    "        net.losses[i] = backward(net.layers[i], dldy)\n",
    "    end\n",
    "    dldy\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialNet(Layer[FCLayer([0.608597 0.868805 … 0.344706 0.166482; 0.521523 0.801306 … 0.345923 0.520167; … ; 0.220836 0.366705 … 0.152793 0.399286; 0.272182 0.79883 … 0.727381 0.99018],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0  …  0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0  …  0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0  …  0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),ReLu(1.0,Float64[],Float64[],Float64[]),FCLayer([0.85175 0.163842 … 0.828514 0.91785; 0.92656 0.354392 … 0.52468 0.893171; … ; 0.548751 0.966283 … 0.736235 0.284618; 0.519856 0.931976 … 0.510095 0.857101],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0  …  0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0  …  0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0  …  0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),ReLu(1.0,Float64[],Float64[],Float64[]),FCLayer([0.0552645 0.356208 … 0.78092 0.306351; 0.26897 0.318731 … 0.120199 0.736154; … ; 0.0235129 0.74782 … 0.971163 0.448897; 0.537161 0.379585 … 0.477909 0.718192],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0  …  0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])],CrossEntropyLoss(Float64[],Float64[]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = [\n",
    "    FCLayer(784, 196),\n",
    "    ReLu(),\n",
    "    FCLayer(196, 49),\n",
    "    ReLu(),\n",
    "    FCLayer(49, 10)\n",
    "]\n",
    "criteria = CrossEntropyLoss()\n",
    "net = SequentialNet(layers, criteria)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,784)(60000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition sgd(Main.SequentialNet, Any, Any) in module Main at In[53]:2 overwritten at In[60]:2.\n",
      "WARNING: Method definition sgd(Main.SequentialNet, Any, Any, Float64) in module Main at In[53]:2 overwritten at In[60]:2.\n",
      "WARNING: Method definition train(Main.SequentialNet, Any, Any) in module Main at In[53]:19 overwritten at In[60]:19.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epo 1:\n",
      "1  64\n",
      "(784,) (1,)\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "LoadError: MethodError: no method matching forward(::CrossEntropyLoss, ::Array{Float64,1}, ::Array{Float64,1})\nClosest candidates are:\n  forward(::CrossEntropyLoss, ::Array{Float64,1}, !Matched::Int32) at In[57]:10\n  forward(!Matched::SequentialNet, ::Array{Float64,N}, ::Array{T,N}) at In[58]:11\n  forward(!Matched::FCLayer, ::Array{Float64,1}) at In[55]:14\n  ...\nwhile loading In[60], in expression starting on line 42",
     "output_type": "error",
     "traceback": [
      "LoadError: MethodError: no method matching forward(::CrossEntropyLoss, ::Array{Float64,1}, ::Array{Float64,1})\nClosest candidates are:\n  forward(::CrossEntropyLoss, ::Array{Float64,1}, !Matched::Int32) at In[57]:10\n  forward(!Matched::SequentialNet, ::Array{Float64,N}, ::Array{T,N}) at In[58]:11\n  forward(!Matched::FCLayer, ::Array{Float64,1}) at In[55]:14\n  ...\nwhile loading In[60], in expression starting on line 42",
      "",
      " in forward(::SequentialNet, ::Array{Float64,1}, ::Array{Float64,1}) at ./In[58]:15",
      " in sgd(::SequentialNet, ::Array{Float64,2}, ::Array{Float64,2}, ::Float64) at ./In[60]:7",
      " in train(::SequentialNet, ::Array{Float64,2}, ::Array{Float64,1}) at ./In[60]:30"
     ]
    }
   ],
   "source": [
    "function sgd(net::SequentialNet, batch_X, batch_Y, lr::Float64 = 0.001)\n",
    "    batch_size = size(batch_X)[1]\n",
    "    ttl_loss   = 0.\n",
    "    for b = 1:batch_size\n",
    "        X, Y = batch_X[b,:], batch_Y[b,:]\n",
    "        println(\"$(size(X)) $(size(Y))\")\n",
    "        loss = forward(net, X, Y) # Propogate the input and output, calculate the loss\n",
    "        backward(net, Y) # Propagate the dldy\n",
    "        for l = 1:length(net.layers)\n",
    "            layer = net.layers[l]\n",
    "            setParam(layer, getParam(layer) - lr * gradient(layer) / batch_size )\n",
    "        end\n",
    "        ttl_loss += loss\n",
    "    end\n",
    "    ttl_loss\n",
    "end\n",
    "\n",
    "function train(net::SequentialNet, X, Y)\n",
    "    batch_size, N = 64, size(Y)[1]\n",
    "    batch=0\n",
    "    for epo = 1:100\n",
    "        println(\"Epo $(epo):\")\n",
    "        for bid = 0:ceil(length(X)/batch_size)-1\n",
    "            batch += 1\n",
    "            sidx::Int = convert(Int64, bid*batch_size+1)\n",
    "            eidx::Int = convert(Int64, min(N, (bid+1)*batch_size))\n",
    "            println(\"$(sidx)  $(eidx)\")\n",
    "            batch_X = X[sidx:eidx,:]\n",
    "            batch_Y = Y[sidx:eidx,:]\n",
    "            loss = sgd(net, batch_X, batch_Y)\n",
    "            println(\"[Epo $(epo) : batch $(batch_id)]: loss = $(loss)\")\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "if size(trainX)[1] != 60000\n",
    "    trainX = trainX'\n",
    "end\n",
    "@assert size(trainX)[1] == size(trainY)[1]\n",
    "println(size(trainX), size(trainY))\n",
    "\n",
    "train(net, trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: UndefVarError: wrong not defined\nwhile loading In[35], in expression starting on line 1",
     "output_type": "error",
     "traceback": [
      "LoadError: UndefVarError: wrong not defined\nwhile loading In[35], in expression starting on line 1",
      ""
     ]
    }
   ],
   "source": [
    "wrong()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epo 1:\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "LoadError: ArgumentError: invalid index: 1.0\nwhile loading In[21], in expression starting on line 1",
     "output_type": "error",
     "traceback": [
      "LoadError: ArgumentError: invalid index: 1.0\nwhile loading In[21], in expression starting on line 1",
      "",
      " in macro expansion at ./multidimensional.jl:350 [inlined]",
      " in macro expansion at ./cartesian.jl:64 [inlined]",
      " in macro expansion at ./multidimensional.jl:348 [inlined]",
      " in _unsafe_getindex! at ./multidimensional.jl:340 [inlined]",
      " in macro expansion at ./multidimensional.jl:298 [inlined]",
      " in _unsafe_getindex(::Base.LinearFast, ::Array{Float64,2}, ::FloatRange{Float64}, ::Colon) at ./multidimensional.jl:291",
      " in trainer(::SequentialNet, ::Array{Float64,2}, ::Array{Float64,1}) at ./In[19]:22"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1,2],(5,),1,5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,3,4]\n",
    "a[1:2], size(a), ndims(a), length(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
